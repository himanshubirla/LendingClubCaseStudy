{
 "cells": [
  {
   "cell_type": "raw",
   "id": "271d883c",
   "metadata": {},
   "source": [
    "!pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c610be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db020e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc907299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import missingno as msno\n",
    "import re\n",
    "from scipy.stats import mode\n",
    "import spacy\n",
    "import tqdm\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.matcher import Matcher\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk import TreebankWordTokenizer,WordPunctTokenizer\n",
    "from nltk.tokenize import word_tokenize,PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b9567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_column\",500)\n",
    "pd.set_option(\"display.max_rows\",500)\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a750af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data=pd.read_csv(r\"E:\\Python\\Msc-AI-Projects\\loan.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7395ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create meta data of the data\n",
    "def create_Meta(data):\n",
    "    meta_data={'dtypes':data.dtypes,\n",
    "              'missing_values':round(data.isna().sum()*100/data.shape[0],3),\n",
    "              'count':[data[i].count() for i in data.columns.values],\n",
    "              'Number_Uniques':[data[i].nunique() for i in data.columns.values]}\n",
    "    return pd.DataFrame(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta_Data=create_Meta(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display meta data of our data\n",
    "Meta_Data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting Meta Data on the basis of missing value percentage in descending order\n",
    "Meta_Data=Meta_Data.sort_values(by='missing_values',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slicing Meta Data on basis of missing value percentage which shall be greater than 30%\n",
    "Meta_Data[Meta_Data['missing_values']>30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have noticed that there are many columns which have more than 90 percent missing values\n",
    "# There are columns which have 30% and 64% missing values also . \n",
    "#We will drop columns having missing values more than 50% and for rest we will explore and then take a call\n",
    "missing_value_columns=Meta_Data[Meta_Data['missing_values']>50].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropped missing value columns\n",
    "ft_data=data.drop(missing_value_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d331254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape after and before dropping missing value columns\n",
    "#We notice that 50% approx columns have been dropped\n",
    "ft_data.shape,data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21052268",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(x):\n",
    "    r=r\"[0-9]\"\n",
    "    start,end=re.search(r,x).span()\n",
    "    x=x[start:end+1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx=ft_data[ft_data[\"emp_length\"].notnull()]['emp_length'].index.values.tolist()\n",
    "ft_data['emp_length'].loc[indx,]=ft_data['emp_length'].loc[indx,].map(find_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a029d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "def data_split(a,x,y):\n",
    "    b=a.split(x)[y]\n",
    "    return b\n",
    "\n",
    "ft_data['term']=ft_data.term.apply(data_split,args=(\" \",1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data[\"int_rate\"]=ft_data.int_rate.apply(data_split,args=(\"%\",0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ft_data['emp_length'].loc[indx,]=ft_data['emp_length'].loc[indx,].apply(data_split,args=(\"+\",0))\n",
    "indx=ft_data[ft_data[\"revol_util\"].notnull()]['revol_util'].index.values.tolist()\n",
    "ft_data['revol_util'].loc[indx,]=ft_data['revol_util'].loc[indx,].apply(data_split,args=(\"%\",0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if missing data is not at Random\n",
    "#Creating dichotomous column for missing variable,1->missing,0->not missing\n",
    "ft_data['desc_isMissing']=np.where(ft_data['desc'].isna(),1,0)\n",
    "ft_data['emp_title_isMissing']=np.where(ft_data['emp_title'].isna(),1,0)\n",
    "ft_data['emp_length_isMissing']=np.where(ft_data['emp_length'].isna(),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.loan_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We find that 80% of description of loan provided by borrower is missing for those who have fully paid \n",
    "ft_data[ft_data.desc_isMissing==1]['loan_status'].value_counts(normalize=True).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a332f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We find that 80% of job title provided by borrower is missing for those who have fully paid\n",
    "ft_data[ft_data.emp_title_isMissing==1]['loan_status'].value_counts(normalize=True).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We find that 70% of job tenure provided by borrower is missing for those who have fully paid\n",
    "ft_data[ft_data.emp_length_isMissing==1]['loan_status'].value_counts(normalize=True).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd3bea1",
   "metadata": {},
   "source": [
    "It has been seen that majority of the missing data are of those who have fully paid, that is those who are likely to fully pay the loan amount. Hence we can say that the missing data is not at random. There is a clear pattern which we can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04920825",
   "metadata": {},
   "outputs": [],
   "source": [
    "descMissing_data=ft_data[ft_data.desc_isMissing==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce305ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "descMissing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf19054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(descMissing_data.loan_amnt.mean(),ft_data[ft_data.desc_isMissing==0]['loan_amnt'].mean(),ft_data['loan_amnt'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(descMissing_data.funded_amnt.mean(),ft_data[ft_data.desc_isMissing==0]['funded_amnt'].mean(),ft_data['funded_amnt'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12613cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "descMissing_data.term.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data[ft_data.desc_isMissing==0]['term'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd3af1",
   "metadata": {},
   "source": [
    "We see that distribution of term on the basis of description missing and not missing are similar that is loan applicants prefference for 3 years is twice more than of 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769baa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.int_rate=ft_data.int_rate.astype('float64')\n",
    "descMissing_data.int_rate=descMissing_data.int_rate.astype('float64')\n",
    "print(descMissing_data.int_rate.mean(),ft_data[ft_data.desc_isMissing==0].int_rate.mean(),ft_data.int_rate.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.emp_length=ft_data.emp_length.astype('float64')\n",
    "descMissing_data.emp_length=descMissing_data.emp_length.astype('float64')\n",
    "print(descMissing_data.emp_length.mean(),ft_data[ft_data.desc_isMissing==0].emp_length.mean(),ft_data.emp_length.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.revol_util=ft_data.revol_util.astype('float64')\n",
    "descMissing_data.revol_util=descMissing_data.revol_util.astype('float64')\n",
    "print(descMissing_data.revol_util.mean(),ft_data[ft_data.desc_isMissing==0].revol_util.mean(),ft_data.revol_util.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f90bdc",
   "metadata": {},
   "source": [
    "No clear pattern in interest rate, revol util and employee length seen for missing description and non missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dbd9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try Finding Employee Length based on Title and imputing the missings\n",
    "title_wise_empLen=ft_data.groupby('emp_title',as_index=False,dropna=True)['emp_length'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_title_NA_Length=ft_data[(~ft_data.emp_title.isna())&(ft_data.emp_length.isna())]['emp_title'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.DataFrame()\n",
    "for i in emp_title_NA_Length:\n",
    "    a=pd.concat([a,title_wise_empLen[title_wise_empLen.emp_title==i]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_title=a[~a.emp_length.isna()].emp_title.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84797edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=[]\n",
    "for i in emp_title:\n",
    "    ind=ft_data[(ft_data.emp_title==i)&(ft_data.emp_length.isna())]['emp_length'].index.values[0]\n",
    "    indexes.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ddf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_emp_length_data=ft_data.loc[indexes,:]\n",
    "emp_lnt=a[~a.emp_length.isna()].emp_length.tolist()    \n",
    "missing_emp_length_data.loc[:,'emp_length']=emp_lnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data=pd.concat([ft_data.drop(indexes,axis=0),missing_emp_length_data],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa1ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=ft_data[(ft_data.emp_length.isna())].index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16429668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows where emp_length is NA\n",
    "ft_data.drop(indexes,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545eb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.isna().sum()*100/ft_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb74b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping employee title most frequent based on employee experience\n",
    "#Treating Missing Values of Employee title\n",
    "empTitle_by_Length=ft_data.groupby(\"emp_length\",as_index=False).agg({\"emp_title\":mode})\n",
    "for i in range(len(empTitle_by_Length)):\n",
    "    empTitle_by_Length.emp_title[i]=empTitle_by_Length.emp_title[i].mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed686e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "empTitle_by_Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=ft_data[ft_data.emp_title.isna()].index.values.tolist()\n",
    "empTitle_NAdata=ft_data.loc[indexes,:]\n",
    "empTitle_NAdata[(empTitle_NAdata.emp_length==1.0)&(empTitle_NAdata.emp_title.isna())][\"emp_title\"].replace(np.nan,\"Bank of America\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c109242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "empTitle_NAdata.emp_length.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1315f0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa422d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation of missing values in employee title based on employee length\n",
    "a=pd.DataFrame()\n",
    "for i in empTitle_NAdata.emp_length.unique().tolist():\n",
    "    dummy=empTitle_NAdata[(empTitle_NAdata.emp_length==i)&(empTitle_NAdata.emp_title.isna())]\n",
    "    dummy.emp_title.replace(np.nan,empTitle_by_Length[empTitle_by_Length.emp_length==i].emp_title.values[0],inplace=True)\n",
    "    a=pd.concat([dummy,a],axis=0)\n",
    "    \n",
    "ft_data=pd.concat([ft_data.drop(indexes,axis=0),a],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cac67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.isna().sum()*100/ft_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data['pub_rec_bankruptcies'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfe6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.groupby('pub_rec_bankruptcies',dropna=False).agg({'loan_status':np.count_nonzero})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e2415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We notice the 697 missing public record of bankcruptcies are not the ones which are either charged off or who have fully paid..\n",
    "#They belong to Current loan applicants\n",
    "#Either this NaN can be another category itself or they belong to one specific category\n",
    "ft_data[ft_data['pub_rec_bankruptcies'].isna()].groupby(\"loan_status\",dropna=False)[\"pub_rec_bankruptcies\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding the behavior of bunkcruptcies on basis of average loan amount\n",
    "#We see that those who have frequently shown bankcruptcies are the ones who have applied for high loan amounts. \n",
    "#The missing 697 are the ones which have applied for least loan amount\n",
    "ft_data.groupby('pub_rec_bankruptcies',dropna=False).agg({'loan_amnt':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding the behavior of bunkcruptcies on basis of average funded amount\n",
    "#There is difference in loan applied amount and funded amount\n",
    "#We see that those who do not have bankcruptcies or who have maximum record of bankcruptcies that is 2 have the highest amount funded. \n",
    "#The missing 697 are the ones which have got lesser funded amount\n",
    "ft_data.groupby('pub_rec_bankruptcies',dropna=False).agg({'funded_amnt':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0605c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding the behavior of bunkcruptcies on basis of average loan amount and loan status\n",
    "#We see for all the number of bankcruptcies, people who have fully paid are the ones who have applied for the least amount and those who are current have the highest loan amount.\n",
    "#For NAN category those whose loan is being charged off has the highest loan amount \n",
    "ft_data.groupby(['pub_rec_bankruptcies','loan_status'],dropna=False).agg({'loan_amnt':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding the behavior of bunkcruptcies on basis of average loan amount and loan status\n",
    "#We see for all the number of bankcruptcies, people who have fully paid are in majority numbers and the current applicants are the least\n",
    "#Of NaNs i.e 697 applicants 83% are already paid off and only 17% have been charged of\n",
    "ft_data.groupby(['pub_rec_bankruptcies','loan_status'],dropna=False).agg({'loan_amnt':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding the behavior of bunkcruptcies on basis of average interest rate\n",
    "#We see that as the number of bankruptcies increases, it increases when the average interest rate rises. \n",
    "#The missing 697 have the least interest rate @11% approximately\n",
    "ft_data.groupby('pub_rec_bankruptcies',dropna=False).agg({'int_rate':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e22e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding the behavior of bunkcruptcies on basis of average loan amount and installment amount\n",
    "#We see for all the number of bankcruptcies, people who have public record of bankcruptcies as two, pays highest isntallment\n",
    "#For NAN category the installment is lesser comparatively\n",
    "ft_data.groupby(['pub_rec_bankruptcies'],dropna=False).agg({'installment':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see for zero and NaN category of the public record of bankcruptcies, majority of the applicants have home ownership as Rent and then Mortgage\n",
    "#Those who displays bankcruptcies have higher frequencies of martgage as home ownership\n",
    "ft_data.groupby([\"pub_rec_bankruptcies\",\"home_ownership\"],dropna=False).agg({\"emp_title\":\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9394e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the patterns seen above we see lot of similarities in NaN category and bankcruotcies class 0 , which is suggestive of that applicants who are having public_bankcrupticies as NaN have zero bankcruptcies\n",
    "#Hence We can impute NaNs with 0\n",
    "ft_data.pub_rec_bankruptcies.fillna(0.0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069041d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.isna().sum()*100/ft_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e468bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ft_data[~ft_data.desc.isna()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96cfa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.desc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85555dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    if len(str(text))>0:\n",
    "    \n",
    "        nlp=spacy.load(\"en_core_web_md\")\n",
    "        stop_words=nlp.Defaults.stop_words\n",
    "        text=[word for word in str(text).split(\" \") if word.lower() not in stop_words]\n",
    "        return \" \".join([i for i in text])\n",
    "    return \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debbd79c",
   "metadata": {},
   "source": [
    "ft_data['cleaned_desc']=ft_data['desc'].map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fa202c",
   "metadata": {},
   "source": [
    "cleaned_desc=[]\n",
    "for i in tqdm.tqdm(ft_data['desc']):\n",
    "    cleaned_desc.append(remove_stopwords(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7046724",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data['cleaned_desc']=ft_data['cleaned_desc'].map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data['cleaned_desc'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.to_pickle(r\"E:\\Python\\Msc-AI-Projects\\ft_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49302f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data=pd.read_pickle(r\"E:\\Python\\Msc-AI-Projects\\ft_data.pkl\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e97f055e",
   "metadata": {},
   "source": [
    "with open(\"Cleaned_desc.pkl\",\"wb\") as f:\n",
    "    pickle.dump(cleaned_desc,f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82212bcb",
   "metadata": {},
   "source": [
    "with open(\"Cleaned_desc.pkl\",\"rb\") as f:\n",
    "    cleaned_desc=pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f74b4ef",
   "metadata": {},
   "source": [
    "ft_data['cleaned_desc']=cleaned_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e02cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove HTML tags\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265f74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data['cleaned_desc']=ft_data['cleaned_desc'].map(strip_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f05bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data['cleaned_desc']=ft_data['cleaned_desc'].map(lambda x: str(x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e0bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_md\")\n",
    "def remove_special_characters(text):\n",
    "    remove_digits=False\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1133f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data['cleaned_desc']=ft_data['cleaned_desc'].map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3f3ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(text):\n",
    "    nlp=spacy.load(\"en_core_web_md\")\n",
    "    if len(text.split())>=2:\n",
    "        text=text.lower()\n",
    "        text=nlp(text)\n",
    "        text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in tq(text)])\n",
    "        \n",
    "        return text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff43853",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data['cleaned_desc']=ft_data['cleaned_desc'].map(lemmatize_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(ft_data['cleaned_desc'][0])\n",
    "for tok in doc:\n",
    "    print(tok.text,\" ----> \",tok.dep_,\" ----> \",tok.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7051fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data['cleaned_desc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b804be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "    \n",
    "    ## chunk 1\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    #############################################################\n",
    "\n",
    "    for tok in nlp(sent):\n",
    "        \n",
    "        ## chunk 2\n",
    "        # if token is a punctuation mark then move on to the next token\n",
    "        if tok.dep_ != \"punct\":\n",
    "          # check: token is a compound word or not\n",
    "          if tok.dep_ == \"compound\":\n",
    "            prefix = tok.text\n",
    "            # if the previous word was also a 'compound' then add the current word to it\n",
    "            if prv_tok_dep == \"compound\":\n",
    "                prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "          # check: token is a modifier or not\n",
    "          if tok.dep_.endswith(\"mod\") == True:\n",
    "            modifier = tok.text\n",
    "            # if the previous word was also a 'compound' then add the current word to it\n",
    "            if prv_tok_dep == \"compound\":\n",
    "                modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "          ## chunk 3\n",
    "          if tok.dep_.find(\"subj\") == True:\n",
    "            ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "            prefix = \"\"\n",
    "            modifier = \"\"\n",
    "            prv_tok_dep = \"\"\n",
    "            prv_tok_text = \"\"      \n",
    "\n",
    "          ## chunk 4\n",
    "          if tok.dep_.find(\"obj\") == True:\n",
    "            ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "\n",
    "          ## chunk 5  \n",
    "        # update variables\n",
    "        prv_tok_dep = tok.dep_\n",
    "        prv_tok_text = tok.text\n",
    "      #############################################################\n",
    "    return [ent1.strip(), ent2.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e63c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_pairs = []\n",
    "\n",
    "for i in tqdm.tqdm(ft_data['cleaned_desc']):\n",
    "    entity_pairs.append(get_entities(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad41c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "    doc = nlp(sent)\n",
    "    # Matcher class object \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    #define the pattern \n",
    "    pattern = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}]\n",
    "    matcher.add(\"matching_1\",[pattern]) \n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) \n",
    "    if k<1:\n",
    "        return \" \"\n",
    "    else:\n",
    "        span = doc[matches[k][1]:matches[k][2]] \n",
    "        return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3856af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [get_relation(i) for i in tqdm.tqdm(ft_data['cleaned_desc'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cdba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [i[0] for i in entity_pairs]\n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in entity_pairs]\n",
    "\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64dfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a directed-graph from a dataframe\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96da0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"added\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eed540",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(ft_data['cleaned_desc'][0])\n",
    "for tok in doc:\n",
    "    print(tok.text,\" ----> \",tok.dep_,\" ----> \",tok.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ff74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722fa80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
